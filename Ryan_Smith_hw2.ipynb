{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryans94/CS584/blob/main/Ryan_Smith_hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kk-RV36-4XH"
      },
      "source": [
        "# CS 584 :: Data Mining :: George Mason University :: Fall 2025\n",
        "\n",
        "\n",
        "# Homework 2: Linear Regression&Neural Networks\n",
        "\n",
        "- **100 points [6% of your final grade]**\n",
        "- **Due Sunday, October 19 by 11:59pm**\n",
        "\n",
        "- *Goals of this homework:* (1) implement the linear regression model; (2) implement the multi-layer perceptron neural network; (3) tune the hyperparameters of MLP model to produce classification result as good as possible.\n",
        "\n",
        "- *Submission instructions:* for this homework, you should submit your notebook file to **Canvas** (look for the homework 2 assignment there). Please name your submission **FirstName_Lastname_hw2.ipynb**, so for example, my submission would be something like **Ziwei_Zhu_hw2.ipynb**. Your notebook should be fully executed so that we can see all outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-5fsoyf-4XJ"
      },
      "source": [
        "## Part 1: Linear Regression (40 points)\n",
        "\n",
        "Recent studies have found that novel mobile games can lead to increased physical activity. A notable example is Pokemon Go, a mobile game combining the Pokemon world through augmented reality with the real world requiring players to physically move around. Specifically, in the following study, researchers have found that Pokemon Go leads to increased levels of physical activity for the most engaged players! https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5174727/\n",
        "![image.png](attachment:image.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMPLM3jv-4XJ"
      },
      "source": [
        "In this part, our goal is to predict the combat point of each pokemon in the 2017 Pokemon Go mobile game. Each pokemon has its own unique attributes that can help predicting its combat points. These include:\n",
        "\n",
        "- Stamina\n",
        "- Attack value\n",
        "- Defense value\n",
        "- Capture rate\n",
        "- Flee rate\n",
        "- Spawn chance\n",
        "- Primary strength\n",
        "\n",
        "The file pokemon_data.csv contains data of 146 pokemons to be used in this homework. The rows of these files refer to the data samples (i.e., pokemon samples), while the columns denote the name of the pokemon (column 1), its attributes (columns 2-8), and the combat point outcome (column 9). You can ignore column 1 for the rest of this problem.\n",
        "\n",
        "First, let's load the data by executing the following code.\n",
        "\n",
        "**Note: you need to install the pandas library beforehand**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSVIkOZq-4XJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data_frame = pd.read_csv('pokemon_data.csv')\n",
        "data_frame.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnqDiBQq-4XK"
      },
      "source": [
        "Pandas is a fast, powerful, flexible and easy-to-use open-source data analysis and manipulation tool, built on top of Python. By executing the following code, let's create one Numpy array to contain the feature data without the name column and one array to contain the combat point ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poawJj5E-4XK"
      },
      "outputs": [],
      "source": [
        "features = data_frame.values[:, 1:-1]\n",
        "labels = data_frame.values[:, -1]\n",
        "print('array of labels: shape ' + str(np.shape(labels)))\n",
        "print('array of feature matrix: shape ' + str(np.shape(features)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCdJwNnF-4XL"
      },
      "source": [
        "Now, you may find out that we have a categorical feature 'primary_strength' in our data. Categorical features require special attention because usually they cannot be the input of regression models as they are. A potential way to treat categorical features is to simply convert each value of the feature to a separate number. However, this might impute non-existent relative associations between the features, which might not always be representative of the data (e.g., if we assign “1” to the value “green” and “2” to the value “red”, the regression algorithm will assume that “red” is greater than “green,” which is not necessarily the case). For this reason, we can use a “one hot encoding” to represent categorical features. According to this, we will create a binary column for each category of the categorical feature, which will take a value of 1 if the sample belongs to that category, and 0 otherwise. For each categorical feature of the problem, count the number of different values and implement the one hot encoding. For the remaining of the problem, you will be working with the one hot encoding of the categorical features.\n",
        "\n",
        "\n",
        "In the next cell, write your code to replace the categorical feature 'primary_strength' with **one-hot encoding** and generate the new version of the Numpy array 'features'.\n",
        "\n",
        "**Hint: if you don't remember one hot encoding, review the slides of our first-week lecture.**\n",
        "\n",
        "**Note: do not use sklearn to automatically generate one hot encoding.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL0t3E9n-4XL"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZF4ZXPj-4XL"
      },
      "source": [
        "Besides, you may also notice that other features have different scales. So, you need to standardize them: $({x-\\mu})/{\\sigma}$, where $\\mu$ is the mean and $\\sigma$ is the standard deviation. Write your code below.\n",
        "\n",
        "**Hint: details about feature standardization is also in slides of our first-week lecture.**\n",
        "\n",
        "**Note: You do not need to do standardize for one-hot encodings.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llOh3Cvk-4XM"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBCgFKE4-4XM"
      },
      "source": [
        "Now, in the next cell, you need to implement your own **linear regression model** using the **Ordinary Least Squares (OLS)** solution **without regularization**. And here, you should adopt the **5-fold cross-validation** method. For each fold compute and print out the **square root** of the residual sum of squares error (RSS) between the actual and predicted outcome variable. Also compute and print out the **average** square root of the RSS over all folds.\n",
        "\n",
        "**Note: You should implement the algorithm by yourself. You are NOT allowed to use Machine Learning libraries like Sklearn.**\n",
        "\n",
        "**Hint: Use numpy.linalg.pinv() for calculating the inverse of a matrix.**\n",
        "\n",
        "**Hint: details about cross-validation is in slides of KNN lecture.**\n",
        "\n",
        "**Hint: the sqrt_RSS for each fold should be 0~3000**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9xda0Pa-4XM"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNWRy5yU-4XM"
      },
      "source": [
        "At the end in this part, please repeat the same experiment as in the previous step, but instead of linear regression, implement linear regression **with L2-norm regularization**. Experiment and report your results (average square root of RSS over 5-fold cross-validation) with different values of the regularization term $\\lambda=\\{1, 0.1, 0.01, 0.001, 0.0001\\}$.\n",
        "\n",
        "**Hint: details about the closed-form solution with regularization is on page 74 in slides of our linear regression lecture.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j0Ugm-p-4XM"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI1uGDcN-4XN"
      },
      "source": [
        "## Part 2: Neural Networks (40 points)\n",
        "\n",
        "In this part, you are going to implement your multi-layer perceptron model by the Pytorch library. You will still use the same handwritten digit image dataset from HW1. So, in the next few cells, please run the provided code to load and process the data, and create dataset objects for further use by Pytorch.\n",
        "\n",
        "**Note: you need to install Pytorch beforehand (check out https://pytorch.org/get-started/locally/). Or, you can use Google Colab for this homework, which is recommended.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNR9Ueii-4XN"
      },
      "outputs": [],
      "source": [
        "# load data from file and split into training and validation sets\n",
        "import numpy as np\n",
        "data = np.loadtxt(\"train.txt\", delimiter=',')\n",
        "perm_idx = np.random.permutation(data.shape[0])\n",
        "vali_num = int(data.shape[0] * 0.2)\n",
        "vali_idx = perm_idx[:vali_num]\n",
        "train_idx = perm_idx[vali_num:]\n",
        "train_data = data[train_idx]\n",
        "vali_data = data[vali_idx]\n",
        "train_features = train_data[:, 1:].astype(np.float32)\n",
        "train_labels = train_data[:, 0].astype(int)\n",
        "vali_features = vali_data[:, 1:].astype(np.float32)\n",
        "vali_labels = vali_data[:, 0].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iomB0_ET-4XN"
      },
      "outputs": [],
      "source": [
        "# define a Dataset class\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx, :], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEE0vloM-4XN"
      },
      "outputs": [],
      "source": [
        "training_data = MNISTDataset(train_features, train_labels)\n",
        "vali_data = MNISTDataset(vali_features, vali_labels)\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "vali_dataloader = DataLoader(vali_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in train_dataloader:\n",
        "    print(f\"Shape of X [N, F]: {X.shape} {X.dtype}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVXKrZLy-4XN"
      },
      "source": [
        "Now, you should have the train_dataloader and vali_dataloader. Then, you need to build and train your multi-layer perceptron model by Pytorch.\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html gives a comprehansive example how to achieve this. Please read this tutorial closely, and implement the model in the next few cells.\n",
        "\n",
        "https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/c30c1dcf2bc20119bcda7e734ce0eb42/quickstart_tutorial.ipynb provides the interactive version, which you can run and edit.\n",
        "\n",
        "**Note: in your implementation:**\n",
        "- you will only have three layers [784 -> 512 -> 10], you need to remove the [512 -> 512] layer in the tutorial.\n",
        "- add 'weight_decay=1e-4' in torch.optim.SGD to add L2 regularization.\n",
        "- train the model for 10 epochs instead of 5 epochs.\n",
        "- keep all other hyper-parameters the same as used in the tutorial.\n",
        "- **You are allowed to resue the code in the tutorial for this homework**\n",
        "\n",
        "\n",
        "**Note: print out the training process and the final accuracy on the validation set.**\n",
        "\n",
        "**Note: you can use Colab for running the code with GPU for free (open a colab notebook, then Runtime->Change runtime type->Hardware accelerator->GPU)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ra77Ohj-4XN"
      },
      "outputs": [],
      "source": [
        "# Write your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIbbQ3yX-4XO"
      },
      "source": [
        "## Part 3: Tune Hyperparameter (20 points)\n",
        "\n",
        "In this part, you need to do your best to tune the hyperparameters in the MLP to build the best model and evaluate the predictions for the testing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noK-O2el-4XO"
      },
      "source": [
        "Now, you should tune four hyperparameters:\n",
        "\n",
        "- the number of layers and the dimension of each layer (explore as much as you can, but choose reasonable settings considering the computational resources you have)\n",
        "- the activation function (choose from sigmoid, tanh, relu)\n",
        "- weight decay\n",
        "- number of training epochs\n",
        "\n",
        "\n",
        "**Hint: You can tune these hyperparameters by one randomly generated validation set, or you can also use the cross-validation method.**\n",
        "\n",
        "**Note: you can use Colab for running the code with GPU for free**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9IP9h9B-4XO"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy4JNtUn-4XO"
      },
      "source": [
        "### Question: What is your final hyperparameter setting? How do you tune them? What choices have you tried?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UmcyYbi-4XO"
      },
      "source": [
        "#### Write your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "c9wWd0Aq-4XO"
      },
      "source": [
        "Please apply the model with the best hyperparameter setting you find above to the testing set. First of all, let's load the testing data by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmPSybTn-4XO"
      },
      "outputs": [],
      "source": [
        "test_features = np.loadtxt(\"test.txt\", delimiter=',')\n",
        "test_labels = np.loadtxt(\"test_label.txt\")\n",
        "print('array of testing feature matrix: shape ' + str(np.shape(test_features)))\n",
        "print('array of testing labels: shape ' + str(np.shape(test_labels)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dss_5iQ3-4XO"
      },
      "source": [
        "Now, report the Accuracy of your best model on the testing set.\n",
        "\n",
        "**Hint: use the following two lines of code to generate the label predictions for test data:**\n",
        "- raw_pred = model(torch.tensor(test_features).to(device).float())\n",
        "- pred = np.argmax(raw_pred.to('cpu').detach().numpy(), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O85C60IZ-4XO"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}