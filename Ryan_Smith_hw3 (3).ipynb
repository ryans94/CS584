{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 584 :: Data Mining :: George Mason University :: Fall 2025\n",
    "\n",
    "\n",
    "# Homework 3: Clustering\n",
    "\n",
    "- **100 points [6% of your final grade]**\n",
    "- **Due Sunday, November 16 by 11:59pm**\n",
    "\n",
    "- *Goals of this homework:* (1) implement your K-means model; (2) implement K-means++; (3) Apply PCA to K-Means.\n",
    "\n",
    "- *Submission instructions:* for this homework, you should submit your notebook file to **Canvas** (look for the homework 3 assignment there). Please name your submission **FirstName_Lastname_hw3.ipynb**, so for example, my submission would be something like **Ziwei_Zhu_hw3.ipynb**. Your notebook should be fully executed so that we can see all outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: K-Means (60 points)\n",
    "\n",
    "In this part, you will implement your own K-means algorithm to conduct clustering on handwritten digit images. In this homework, we will still use the handwritten digit image dataset we have already used in previous homework. However, since clustering is unsupervised learning, which means we do not use the label information for model training anymore. So, here, we will only use the testing data stored in the \"test.txt\" file.\n",
    "\n",
    "First, let's load the data by executing the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of testing feature matrix: shape (10000, 784)\n",
      "array of testing label matrix: shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test = np.loadtxt(\"test.txt\", delimiter=',')\n",
    "test_features = test[:, 1:]\n",
    "test_labels = test[:, 0]\n",
    "print('array of testing feature matrix: shape ' + str(np.shape(test_features)))\n",
    "print('array of testing label matrix: shape ' + str(np.shape(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time for you to implement your own K-means algorithm. First, please write your code to build your K-means model using the image data with **K = 10**, and **Euclidean distance**.\n",
    "\n",
    "**Note: You should implement the algorithm by yourself. You are NOT allowed to use Machine Learning libraries like Sklearn**\n",
    "\n",
    "**Note: you need to decide when to stop the iteration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|████████████████████████████████████████████████████▋                            | 65/100 [00:56<00:30,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "from tqdm import tqdm\n",
    "\n",
    "def euc_dist(data,cluster):\n",
    "    return np.sqrt(np.sum((data-cluster)**2))\n",
    "\n",
    "def cent_comp(data,cluster,centroid,k):\n",
    "    d=data.shape[1]\n",
    "    new_centroids=np.empty((k,d), dtype=float)\n",
    "    for i in range(k):\n",
    "        points=(data[cluster==i])\n",
    "        if(points.size==0):\n",
    "            new_centroids[i]=centroid[i]\n",
    "        else:\n",
    "            new_centroids[i]=points.mean(axis=0)\n",
    "    return new_centroids\n",
    "\n",
    "centroid_idx=np.random.permutation(test_features.shape[0])[:10]\n",
    "cent=np.array([test_features[idx] for idx in centroid_idx],dtype=float)\n",
    "clust=np.empty(len(test_features), int)\n",
    "def k_means(k, iter,tol,centroid,cluster):\n",
    "    \n",
    "    #print(euc_dist(centroid[1],test_features[0])) # now centroid is initialized as list of 10 random features from test features\n",
    "    \n",
    "    for j in tqdm(range(iter)):\n",
    "        \n",
    "        for i,data in enumerate(test_features):\n",
    "            k_dist=euc_dist(data,centroid[0])\n",
    "            k_cluster=0\n",
    "            for idx in range(1, len(centroid)):\n",
    "                i_dist=euc_dist(data,centroid[idx])\n",
    "                if(i_dist<k_dist):\n",
    "                    k_dist=i_dist\n",
    "                    k_cluster=idx\n",
    "            cluster[i]=k_cluster\n",
    "        \n",
    "        x=test_features\n",
    "        new_centroid=cent_comp(x,cluster,centroid,k)\n",
    "        diff=np.sum(abs(new_centroid-centroid))\n",
    "        #diff=euc_dist(new_centroid,centroid)\n",
    "        if(diff<tol):\n",
    "            centroid=new_centroid\n",
    "            break\n",
    "        #print(f'distance between old/new: {euc_dist(new_centroid,centroid)}')\n",
    "        centroid=new_centroid\n",
    "        #print(f'test {j}')\n",
    "    return centroid,cluster\n",
    "cent,clust=k_means(10,100,1e-10,cent,clust)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to calculate and print the **square root** of **S**um of **S**quared **E**rror (SSE) of each cluster generated by your K-means algorithm. Then, print out the averaged square root of SSE over all clusters.\n",
    "\n",
    "**Note: The value of \"square root of SSE\" can diverge significantly. The expected value range is 10000~100000.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For cluster 0: 52623.02270738569\n",
      "For cluster 1: 61252.46806774508\n",
      "For cluster 2: 52415.45838948367\n",
      "For cluster 3: 49370.00291654311\n",
      "For cluster 4: 58800.54975634594\n",
      "For cluster 5: 50226.538306916096\n",
      "For cluster 6: 50908.591569024975\n",
      "For cluster 7: 34516.27744916376\n",
      "For cluster 8: 49669.45108077166\n",
      "For cluster 9: 37193.69839221044\n",
      "\n",
      "Average SSE for all clusters is: 49697.60586355904\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "k=cent.shape[0]\n",
    "total=0\n",
    "for i in range(k):\n",
    "    i_pts=test_features[clust==i]\n",
    "    sse=np.sqrt(np.sum((i_pts-cent[i])**2))\n",
    "    total+=sse\n",
    "    print(f'For cluster {i}: {sse}')\n",
    "print(f'\\nAverage SSE for all clusters is: {total/k}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, please have a look on https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_completeness_v_measure.html#sklearn.metrics.homogeneity_completeness_v_measure, and use this function to print out the homogeneity, completeness, and v-measure of your K-means model.\n",
    "\n",
    "**Note: The values of homogeneity, completeness, and v-measure are expected to be >0.48**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.49912264364737735\n",
      "Completeness: 0.5025963389299022\n",
      "V-Measure: 0.500853468362487\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "from sklearn.metrics import homogeneity_completeness_v_measure\n",
    "\n",
    "hcv=[]\n",
    "hcv=homogeneity_completeness_v_measure(test_labels, clust)\n",
    "print(f'Homogeneity: {hcv[0]}\\nCompleteness: {hcv[1]}\\nV-Measure: {hcv[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: K-Means++ (30 points)\n",
    "\n",
    "Ok, now you already have a good model. But can you further improve it? In the next cell, please implement the K-means++ model introduced in the lecture.\n",
    "\n",
    "**Note: Everything else is the same as Part1, i.e., K=10, and you need to use Euclidean distance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████████████████████████████████████████▎                                       | 51/100 [00:44<00:42,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# reset centroid/cluster arrays\n",
    "cent=np.zeros((10,test_features.shape[1]))\n",
    "clust=np.empty(len(test_features), int)\n",
    "\n",
    "#initialize first cluster w/ random point in data, and min_dist array with inf, to capture min distances between point and centroid\n",
    "min_dist=np.full(test_features.shape[0],np.inf)\n",
    "rand_idx=np.random.choice(test_features.shape[0])\n",
    "cent[0]=test_features[rand_idx]\n",
    "\n",
    "k=10\n",
    "#loop for k++ initialization\n",
    "for j in range(1,k):\n",
    "    for i in range(len(test_features)):\n",
    "        d=euc_dist(test_features[i],cent[j-1])\n",
    "        if d<min_dist[i]:\n",
    "            min_dist[i]=d\n",
    "    probs=min_dist/np.sum(min_dist)\n",
    "    idx=np.random.choice(len(test_features),p=probs)\n",
    "    cent[j]=test_features[idx]\n",
    "print(len(cent))\n",
    "\n",
    "#run k-means with empty cluster and newly k++ initialized centroids\n",
    "cent,clust=k_means(10,100,1e-10,cent,clust)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, use sklearn.metrics.homogeneity_completeness_v_measure() to print out the homogeneity, completeness, and v-measure of your K-means++ model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5128245265444459 0.5185967310678528 0.5156944771393042\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "hcv_2=homogeneity_completeness_v_measure(test_labels, clust)\n",
    "print(hcv_2[0],hcv_2[1],hcv_2[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dimension Reduction by PCA (10 points)\n",
    "\n",
    "Last, in this part, please use PCA to reduce the feature dimension here. And then, apply your K-Means code to the reduced data and report homogeneity, completeness, and v-measure.\n",
    "\n",
    "**Note: You need to consider the reduced dimension m of PCA as a hyper-parameter to tune. That is, you need to try different m and measure the corresponding clustering performance. At the end, you need to report the best m and clustering performance.**\n",
    "\n",
    "**Note: Everything else is the same as Part1, i.e., K=10, and you need to use Euclidean distance.**\n",
    "\n",
    "**Note: You can reuse your own code of PCA from your HW1, or you can directly use the PCA model from sklearn -- https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|█████████████████████████████████████████████████████████████████▌               | 81/100 [00:56<00:13,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#150 features provided good metrics \n",
    "pca=PCA(200)\n",
    "pca.fit(test_features)\n",
    "reduced_features=pca.transform(test_features)\n",
    "print(reduced_features.shape[0])\n",
    "test_features=reduced_features\n",
    "\n",
    "#reset centroid and cluster to empty and a random permutation of features beased on reduced feature set\n",
    "centroid_idx=np.random.permutation(test_features.shape[0])[:k]\n",
    "cent=test_features[centroid_idx].astype(float)\n",
    "clust=np.empty(len(test_features), int)\n",
    "                                \n",
    "cent,clust=k_means(10,100,1e-10,cent,clust)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, use sklearn.metrics.homogeneity_completeness_v_measure() to print out the homogeneity, completeness, and v-measure of your K-means model.\n",
    "\n",
    "**Note: The values of homogeneity, completeness, and v-measure are expected to be >0.48**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5213249904417651 0.5359245861183178 0.5285239851212393\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "hcv_3=homogeneity_completeness_v_measure(test_labels, clust)\n",
    "print(hcv_3[0],hcv_3[1],hcv_3[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
